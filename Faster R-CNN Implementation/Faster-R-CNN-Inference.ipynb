{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "DIR_INPUT = '/kaggle/input/global-wheat-detection'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST = f'{DIR_INPUT}/test'\n",
    "\n",
    "DIR_WEIGHTS = '/kaggle/input/torchvision-faster-r-cnn-finetuning'\n",
    "WEIGHTS_FILE = f'{DIR_WEIGHTS}/fasterrcnn_resnet50_fpn.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission File\n",
    "\n",
    "The submission format requires a space delimited set of bounding boxes. For example:\n",
    "\n",
    "`ce4833752,0.5 0 0 100 100`\n",
    "\n",
    "indicates that image `ce4833752` has a bounding box with a confidence of `0.5`, at `x` == 0 and `y` == 0, with a `width` and `height` of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>348a992bb</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cb8d261a3</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53f253011</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id PredictionString\n",
       "5  348a992bb    1.0 0 0 50 50\n",
       "6  cc3532ff6    1.0 0 0 50 50\n",
       "7  2fd875eaa    1.0 0 0 50 50\n",
       "8  cb8d261a3    1.0 0 0 50 50\n",
       "9  53f253011    1.0 0 0 50 50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(f'{DIR_INPUT}/sample_submission.csv')\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WheatTest(Dataset):\n",
    "\n",
    "  def __init__(self, image_ids, image_dir, transforms=None):\n",
    "    super().__init__()\n",
    "    self.image_ids = image_ids\n",
    "    self.image_dir = image_dir\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx: int):\n",
    "    image_id = self.image_ids[idx]\n",
    "\n",
    "    image = cv.imread(f'{self.image_dir}/{image_id}.jpg', cv.IMREAD_COLOR)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "\n",
    "    if self.transforms:\n",
    "      sample = {\n",
    "        'image': image,\n",
    "      }\n",
    "      sample = self.transforms(**sample)\n",
    "      image = sample['image']\n",
    "\n",
    "    return image, image_id\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.image_ids)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_test_transform():\n",
    "    return A.Compose([\n",
    "      ToTensorV2(p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(p):\n",
    "  import glob\n",
    "  image_ids = []\n",
    "  for p in glob.glob(f'{p}/*.jpg'):\n",
    "    n, _ = os.path.splitext(os.path.basename(p))\n",
    "    image_ids.append(n)\n",
    "  return image_ids\n",
    "\n",
    "# try more images for submission\n",
    "#test_dataset = WheatTest(get_image_ids(DIR_TRAIN), DIR_TRAIN, WheatTest.get_test_transform())\n",
    "\n",
    "test_dataset = WheatTest(test_df[\"image_id\"].unique(), DIR_TEST, WheatTest.get_test_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return tuple(zip(*batch))\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=8, # GPU not enough if inference 16 images\n",
    "  shuffle=False,\n",
    "  num_workers=4,\n",
    "  drop_last=False,\n",
    "  collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# create a Faster R-CNN model without pre-trained\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "num_classes = 2 # wheat or not(background)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained model's head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# load the trained weights\n",
    "model.load_state_dict(torch.load(WEIGHTS_FILE, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# move model to the right device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Test Image Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.7\n",
    "image_outputs = []\n",
    "\n",
    "for images, image_ids in test_data_loader:\n",
    "  images = list(image.to(device) for image in images)\n",
    "  outputs = model(images)\n",
    "    \n",
    "  for image_id, output in zip(image_ids, outputs):\n",
    "    boxes = output['boxes'].data.cpu().numpy()\n",
    "    scores = output['scores'].data.cpu().numpy()\n",
    "    \n",
    "    mask = scores >= score_threshold\n",
    "    boxes = boxes[mask].astype(np.int32)\n",
    "    scores = scores[mask]\n",
    "\n",
    "    image_outputs.append((image_id, boxes, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 'aac893a91',\n",
       "  'PredictionString': '0.9938 557 531 124 193 0.9923 70 2 101 159 0.9863 613 923 83 99 0.9843 740 772 81 115 0.9834 330 666 115 150 0.9828 696 394 117 173 0.982 820 708 105 198 0.9815 28 454 102 154 0.977 175 566 112 186 0.9766 590 779 94 119 0.9732 357 534 95 81 0.9713 458 863 81 92 0.9599 306 0 73 67 0.9402 236 843 155 106 0.927 550 56 142 196 0.9057 93 620 115 90 0.9043 249 97 126 134 0.8824 67 856 112 70 0.8533 828 629 81 120 0.8187 165 6 80 143 0.7453 825 910 123 112 0.7126 359 263 99 140'},\n",
       " {'image_id': '51f1be19e',\n",
       "  'PredictionString': '0.9768 607 85 162 176 0.9728 842 260 129 207 0.9668 277 479 134 114 0.9656 65 692 127 213 0.9645 815 95 115 76 0.9641 803 761 113 100 0.9614 768 882 148 105 0.9608 186 922 117 101 0.9606 508 473 192 108 0.9565 697 920 85 88 0.9509 24 0 88 72 0.9424 0 381 57 100 0.9281 653 795 104 79 0.9169 353 143 96 175 0.8925 774 26 116 68 0.8735 245 120 113 123 0.8703 565 597 116 122 0.8592 654 583 115 90 0.8252 913 561 97 111 0.8075 861 652 153 109'},\n",
       " {'image_id': 'f5a1f0358',\n",
       "  'PredictionString': '0.9883 539 271 115 116 0.9868 139 747 162 125 0.9849 689 205 113 89 0.9849 881 635 95 153 0.9844 941 433 82 184 0.9828 0 845 72 112 0.9822 151 252 81 87 0.9821 549 408 82 96 0.9808 692 568 84 129 0.9776 290 459 160 104 0.9767 604 728 100 87 0.9742 63 460 142 130 0.974 438 299 123 192 0.9714 216 315 127 104 0.971 815 409 96 90 0.9691 665 108 86 98 0.953 252 673 110 76 0.9477 519 1 114 106 0.9465 395 686 102 152 0.9428 408 177 71 72 0.9029 228 556 104 117 0.9 748 578 117 146 0.8888 475 807 128 88 0.8641 0 1 68 66 0.8485 122 610 80 79'},\n",
       " {'image_id': '796707dd7',\n",
       "  'PredictionString': '0.9821 894 332 114 96 0.9737 673 459 106 119 0.9722 941 75 82 100 0.9697 504 787 93 92 0.9663 712 834 108 95 0.9632 377 632 100 122 0.9573 0 454 72 73 0.9572 616 35 89 79 0.9322 462 273 110 97 0.932 49 88 157 114 0.9235 767 528 190 188 0.8787 52 573 85 82 0.876 114 552 184 110 0.8759 237 325 113 256 0.8701 91 803 111 80 0.8656 195 476 88 96 0.7957 672 33 115 119 0.7752 668 567 118 157 0.7709 294 289 86 126 0.7418 350 0 90 57'},\n",
       " {'image_id': '51b3e36ab',\n",
       "  'PredictionString': '0.9965 1 438 98 321 0.994 233 649 96 154 0.9923 539 29 249 125 0.9913 874 290 149 137 0.9903 0 912 90 110 0.9903 463 23 86 136 0.989 4 827 106 101 0.9876 415 937 90 87 0.9867 832 447 191 148 0.9829 365 155 99 92 0.9818 695 625 326 124 0.9811 339 467 83 152 0.9806 445 325 143 101 0.9798 503 366 303 110 0.9774 611 770 162 98 0.9769 708 818 258 113 0.9736 476 578 80 82 0.9727 876 195 130 76 0.9704 114 850 142 95 0.9673 498 186 98 85 0.9375 390 430 97 76 0.9344 2 1 98 192 0.9333 0 292 81 168 0.7906 97 653 117 78 0.7568 856 719 86 76'},\n",
       " {'image_id': '348a992bb',\n",
       "  'PredictionString': '0.9902 3 316 118 99 0.9891 734 223 138 89 0.9881 593 447 128 102 0.9876 917 563 90 92 0.9868 283 338 83 87 0.9862 538 40 83 95 0.9831 0 472 134 108 0.983 454 661 80 81 0.983 142 41 110 85 0.9809 936 785 85 79 0.9784 301 169 92 99 0.976 758 118 129 88 0.9759 0 942 63 82 0.9757 403 517 87 87 0.9739 374 217 89 82 0.9729 964 673 60 86 0.9725 100 211 92 77 0.9697 858 848 80 71 0.969 111 927 91 75 0.9673 372 860 103 112 0.9649 500 960 119 64 0.9611 659 46 114 113 0.958 464 540 85 88 0.9511 552 747 87 89 0.9495 137 604 78 63 0.9466 728 482 66 77 0.9247 666 386 105 77 0.9061 50 271 71 74 0.8659 6 422 68 67 0.8283 501 714 96 74 0.7983 439 974 67 50 0.7955 745 984 115 40'},\n",
       " {'image_id': 'cc3532ff6',\n",
       "  'PredictionString': '0.9932 768 835 171 162 0.9925 488 583 101 130 0.9896 261 644 105 166 0.9876 75 812 142 166 0.9867 609 434 88 97 0.9852 94 604 93 152 0.9846 910 123 113 101 0.9832 478 404 122 148 0.9804 371 3 87 98 0.9803 555 835 121 171 0.9796 696 473 133 86 0.9764 2 414 150 104 0.9724 559 304 117 112 0.9605 718 300 114 110 0.9508 20 342 103 77 0.9498 776 38 94 95 0.9493 955 6 69 85 0.9182 24 655 81 72 0.907 304 292 95 78 0.8979 785 708 103 108 0.8882 274 841 117 83 0.8792 4 479 71 78 0.7716 0 770 47 74 0.756 407 224 82 83'},\n",
       " {'image_id': '2fd875eaa',\n",
       "  'PredictionString': '0.993 458 505 82 128 0.9925 940 652 82 95 0.9911 0 913 106 82 0.9907 388 798 81 87 0.9906 732 891 96 87 0.9905 463 354 127 101 0.9898 536 877 105 110 0.9887 109 590 135 81 0.9885 0 1 104 78 0.9879 0 740 89 109 0.9869 889 52 105 87 0.9867 909 886 76 93 0.9852 124 849 83 70 0.9829 425 65 106 78 0.981 925 777 92 84 0.9803 730 153 84 89 0.9715 472 13 76 74 0.9707 232 0 94 57 0.9689 117 37 103 71 0.959 787 725 102 77 0.952 98 0 123 59 0.9503 447 968 103 55 0.9467 985 583 39 90 0.8705 433 483 70 75'},\n",
       " {'image_id': 'cb8d261a3',\n",
       "  'PredictionString': '0.9848 307 162 115 204 0.9826 21 866 82 142 0.9767 442 463 112 102 0.9765 27 560 170 102 0.9745 749 715 89 86 0.9735 512 270 96 134 0.973 598 249 80 121 0.9714 588 34 81 86 0.9685 435 117 102 79 0.9633 900 175 102 117 0.9581 650 680 95 74 0.956 462 922 198 98 0.9533 756 489 119 92 0.9529 839 153 76 122 0.9427 687 126 83 145 0.9356 264 775 115 73 0.9346 796 217 68 103 0.9326 0 797 46 84 0.9108 458 817 81 78 0.9084 167 908 90 82 0.8724 529 523 108 60'},\n",
       " {'image_id': '53f253011',\n",
       "  'PredictionString': '0.9959 468 470 158 195 0.9956 931 205 93 131 0.9912 926 810 98 210 0.9881 22 601 120 143 0.9874 235 838 112 94 0.9859 140 918 110 105 0.9855 619 110 119 142 0.9846 784 639 113 107 0.9845 146 102 91 87 0.9839 392 830 109 113 0.9837 14 39 140 106 0.9832 907 747 77 84 0.9815 153 325 135 103 0.9814 306 69 91 126 0.9797 605 705 109 130 0.9793 0 401 100 124 0.9781 452 173 115 119 0.9737 299 443 187 94 0.9717 604 332 115 192 0.9713 578 586 86 133 0.9697 288 611 136 102 0.9637 357 348 159 89 0.9597 137 630 96 115 0.9588 199 571 114 100 0.9575 727 365 147 127 0.9489 413 274 114 95 0.9426 524 398 81 72 0.9366 0 764 109 65 0.7049 175 505 103 81'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "  pred_strings = []\n",
    "  for score, box in zip(scores, boxes):\n",
    "    pred_strings.append(round(score, 4))\n",
    "    pred_strings.extend(box)\n",
    "  return ' '.join(map(str, pred_strings))\n",
    "\n",
    "results = []\n",
    "\n",
    "for image_id, boxes, scores in image_outputs:\n",
    "  #boxes = boxes_.copy()\n",
    "  boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "  boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "  result = {\n",
    "    'image_id': image_id,\n",
    "    'PredictionString': format_prediction_string(boxes, scores)\n",
    "  }\n",
    "  results.append(result)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac893a91</td>\n",
       "      <td>0.9938 557 531 124 193 0.9923 70 2 101 159 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51f1be19e</td>\n",
       "      <td>0.9768 607 85 162 176 0.9728 842 260 129 207 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5a1f0358</td>\n",
       "      <td>0.9883 539 271 115 116 0.9868 139 747 162 125 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>796707dd7</td>\n",
       "      <td>0.9821 894 332 114 96 0.9737 673 459 106 119 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9965 1 438 98 321 0.994 233 649 96 154 0.992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>348a992bb</td>\n",
       "      <td>0.9902 3 316 118 99 0.9891 734 223 138 89 0.98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9932 768 835 171 162 0.9925 488 583 101 130 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>0.993 458 505 82 128 0.9925 940 652 82 95 0.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cb8d261a3</td>\n",
       "      <td>0.9848 307 162 115 204 0.9826 21 866 82 142 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53f253011</td>\n",
       "      <td>0.9959 468 470 158 195 0.9956 931 205 93 131 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  aac893a91  0.9938 557 531 124 193 0.9923 70 2 101 159 0.9...\n",
       "1  51f1be19e  0.9768 607 85 162 176 0.9728 842 260 129 207 0...\n",
       "2  f5a1f0358  0.9883 539 271 115 116 0.9868 139 747 162 125 ...\n",
       "3  796707dd7  0.9821 894 332 114 96 0.9737 673 459 106 119 0...\n",
       "4  51b3e36ab  0.9965 1 438 98 321 0.994 233 649 96 154 0.992...\n",
       "5  348a992bb  0.9902 3 316 118 99 0.9891 734 223 138 89 0.98...\n",
       "6  cc3532ff6  0.9932 768 835 171 162 0.9925 488 583 101 130 ...\n",
       "7  2fd875eaa  0.993 458 505 82 128 0.9925 940 652 82 95 0.99...\n",
       "8  cb8d261a3  0.9848 307 162 115 204 0.9826 21 866 82 142 0....\n",
       "9  53f253011  0.9959 468 470 158 195 0.9956 931 205 93 131 0..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
